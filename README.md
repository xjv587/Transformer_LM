# Transformer_LM

I implemented and trained a Transformer-based language model from scratch using PyTorch. 
- Simplified Transformer Encoder. Implement a Transformer encoder to predict the number of times a character has appeared in a sequence up to that point.
- Transformer Language Model. Develop a Transformer-based language model to predict the next character in a sequence, using a larger dataset derived from Wikipedia text.

# Outcome Achieved
- Achieved 96% accuracy on a character-level classification task, with the opportunity to visualize attention maps and debug model behavior.
- Build a model that achieves a perplexity of â‰¤7, demonstrating the ability to generalize and predict character sequences effectively.

# Key Skills
- Building and training a Transformer model from scratch, understanding self-attention mechanisms, and working with positional encodings.
- mplementing a causal language model, handling sequential data, and optimizing model performance using perplexity as a metric.
